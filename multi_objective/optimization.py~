import numpy as np

from . import EI_tools as EIT
from . import pareto_tools as PT

def get_next_point(X,F,GPRs,bounds,r):
    '''
    get the next evaluation point for X based on expected hypervolume improvement
    -----------------------------------------------------------

    X: array of input space vectors
    F: array of objective space vectors
    GPRs: list of scikit-learn GaussianProcessRegressors that have been trained 
             on (X,F)
    bounds: array specifying input space boundaries
    r: reference point

    -----------------------------------------------------------
    output: point in input parameter space that maximizes EHVI
    '''
    dim = F.shape()[1]

    assert dim == len(GPRs),'# of gaussian processes != objective space!'
    
    if dim == 2:
        S = PT.get_non_dominated_set(F)
        S = PT.sort_along_first_axis(S)[::-1]

        new_point = layered_minimization(get_EHVI, bounds, args = (GPRs,S,r))
        return new_point
        
    elif dim == 3:
        pass
    else:
        print('can\'t do higher dimentional problems yet!')

def get_EHVI(X,GPRs,S,r):
    '''
    x: point input from optimizer
    s: non-dominated set of points
    GPRs: list of GP regressors
    r: reference point
    '''
    #logging.info(f'calling get_EHVI() with point:{x.reshape(-1,2)}')
    f = np.array([ele.predict(X.reshape(-1,2),return_std=True) for ele in GPRs]).T[0]
    #logging.info(f)
    return -EHVI(f[0],f[1],s,r)

        
def layered_minimization(func,bounds,n_restarts = 10, args=()):
    min_val = 10000000
    dim = len(bounds)
    nfev = 0

    s = time.time()
    for x0 in np.random.uniform(bounds[:,0],bounds[:,1], size = (n_restarts,dim)):
        res = minimize(func, x0 = x0, args = args,bounds = bounds, method='L-BFGS-B')
        nfev = nfev + res.nfev
        if res.fun < min_val:
            min_val = res.fun
            min_x = res.x
    logging.info(f'number of function evaluations {nfev}, avg exec time {(time.time() - s)/nfev}')
    return min_x
